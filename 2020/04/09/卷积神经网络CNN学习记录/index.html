<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><!--!--><title>卷积神经网络CNN入门 - ggb0n&#039;s Blog</title><meta description="本文主要是记录一下对卷积神经网络（CNN）的学习，这是在机器学习，具体来说是深度学习中的第一步，认真学习、好好记录。 CNN曾经被AlphaGo的开发团队用于其开发，从而也一度引起业内人士的青睐，不过有些时候会出现对CNN的盲目崇拜，比如你面试的时候，对面可能会问：你做过机器学习吗？你为什么没有采用CNN算法呢？"><meta property="og:type" content="article"><meta property="og:title" content="卷积神经网络CNN入门"><meta property="og:url" content="http://ggb0n.cool/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"><meta property="og:site_name" content="ggb0n&#039;s blog"><meta property="og:description" content="本文主要是记录一下对卷积神经网络（CNN）的学习，这是在机器学习，具体来说是深度学习中的第一步，认真学习、好好记录。 CNN曾经被AlphaGo的开发团队用于其开发，从而也一度引起业内人士的青睐，不过有些时候会出现对CNN的盲目崇拜，比如你面试的时候，对面可能会问：你做过机器学习吗？你为什么没有采用CNN算法呢？"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://ggb0n.cool/images/icon2.png"><meta property="article:published_time" content="2020-04-08T16:16:23.000Z"><meta property="article:modified_time" content="2020-04-16T07:47:14.317Z"><meta property="article:author" content="ggb0n"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Neural Network"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="http://ggb0n.cool/images/icon2.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://ggb0n.cool/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"},"headline":"卷积神经网络CNN入门","image":["http://www.ggb0n.cool/images/back2.png"],"datePublished":"2020-04-08T16:16:23.000Z","dateModified":"2020-04-16T07:47:14.317Z","author":{"@type":"Person","name":"ggb0n"},"description":"本文主要是记录一下对卷积神经网络（CNN）的学习，这是在机器学习，具体来说是深度学习中的第一步，认真学习、好好记录。 CNN曾经被AlphaGo的开发团队用于其开发，从而也一度引起业内人士的青睐，不过有些时候会出现对CNN的盲目崇拜，比如你面试的时候，对面可能会问：你做过机器学习吗？你为什么没有采用CNN算法呢？"}</script><link rel="alternative" href="/atom.xml" title="ggb0n&#039;s Blog" type="application/atom+xml"><link rel="icon" href="http://www.ggb0n.cool/images/icon2.png"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.12/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><link rel="stylesheet" href="/css/style.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><script src="/js/md5.min.js"></script></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo1.png" alt="ggb0n&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/friend">友链</a><a class="navbar-item" href="/message">留言</a><a class="navbar-item" href="/about">关于</a><a class="navbar-item" href="/tools">工具</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="thumbnail" src="http://ggb0n.cool/images/cnn-top.jpg" alt="卷积神经网络CNN入门"></span></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-04-08T16:16:23.000Z">2020-04-09</time><a class="commentCountImg" href="/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/#comment-container"><span class="display-none-class">f8c2b0786d3cfb6dc31c18f16977f3f2</span><img class="not-gallery-item" src="/img/chat.svg"> <span class="commentCount" id="f8c2b0786d3cfb6dc31c18f16977f3f2"> 99+</span>    </a><span class="level-item">an hour read (About 8291 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span> visits</span></div></div><h1 class="title is-3 is-size-4-mobile">卷积神经网络CNN入门</h1><div class="content"><p>本文主要是记录一下对<code>卷积神经网络</code>（<code>CNN</code>）的学习，这是在机器学习，具体来说是深度学习中的第一步，认真学习、好好记录。</p>
<p>CNN曾经被<code>AlphaGo</code>的开发团队用于其开发，从而也一度引起业内人士的青睐，不过有些时候会出现对CNN的盲目崇拜，比如你面试的时候，对面可能会问：你做过机器学习吗？你为什么没有采用CNN算法呢？<a id="more"></a><br>这可能让人无法回答，其实CNN虽然相对于<code>深度神经网络</code>（<code>DNN</code>）更加简便，效能也很好，不过CNN只是用于图像处理，或者具有与图像处理要素相当的一些问题的处理中，就比如<code>AlphaGo</code>的下围棋，其实就相当于把棋盘当做了图像来进行处理，因此，如果你需要利用神经网络实现某些项目，一定要考虑好它具备与图像处理相当的要素没有。</p>
<p>这里提供一个比较好的学习视频，是<a href="https://www.bilibili.com/video/BV1hp411d7ij/?spm_id_from=333.788.videocard.0">李宏毅老师的教学视频</a>。好了，下面进入学习记录了。</p>
<h3 id="卷积神经网络概述"><a href="#卷积神经网络概述" class="headerlink" title="卷积神经网络概述"></a>卷积神经网络概述</h3><p>卷积神经网络，是深度学习算法应用最成功的领域之一，卷积神经网络包括一维卷积神经网络，二维卷积神经网络以及三维卷积神经网络。一维卷积神经网络主要用于<code>序列类</code>的数据处理，二维卷积神经网络常应用于<code>图像类文本</code>的识别，三维卷积神经网络主要应用于<code>医学图像</code>以及<code>视频类</code>数据识别。</p>
<p>其结构模型如下：<br><img src="http://ggb0n.cool/images/moxing1.png" alt=""></p>
<p>与常规神经网络不同，卷积神经网络的各层中的神经元是<code>3维</code>排列的：<code>宽度</code>、<code>高度</code>和<code>深度</code>。其中的宽度和高度是很好理解的，因为本身卷积就是一个二维模板，但是在卷积神经网络中的深度指的是<strong>激活数据体</strong>的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数。</p>
<h3 id="卷积神经网络层次"><a href="#卷积神经网络层次" class="headerlink" title="卷积神经网络层次"></a>卷积神经网络层次</h3><p>卷积神经网络主要由这几类层构成：<code>输入层</code>、<code>卷积层</code>，<code>ReLU层</code>、<code>池化（Pooling）层</code>(有时也称降采样、下采样层)和<code>全连接层</code>（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络，如下图：</p>
<p><img src="http://ggb0n.cool/images/moxing2.png" alt=""></p>
<p>各层的主要作用如下：</p>
<ul>
<li>输入层：用于数据的输入；</li>
<li>卷积层：使用卷积核进行特征提取和特征映射；</li>
<li>池化层：进行下采样，对特征图稀疏处理，减少数据运算量；</li>
<li>激励层：由于卷积也是一种线性运算，因此需要增加非线性映射；</li>
<li>全连接层：通常在CNN的尾部进行重新拟合，减少特征信息的损失。</li>
</ul>
<h4 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h4><p>该层除了输入数据，还会对数据进行一些预处理，包括：<code>去均值</code>、<code>归一化</code>、<code>PCA</code>、<code>白化</code></p>
<blockquote>
<p>PCA：是指通过抛弃携带信息量较少的维度，保留主要的特征信J自，来对数据进行降维处理，思路上是使用少数几个有代表性、互不相关的特征来代替原先的..k量的、存在一定相关性的特征，从而加速机器学习进程。PCA可用于特征提取，数据压缩，去噪声，降维等操作。</p>
</blockquote>
<blockquote>
<p>白化：目的是去掉数据之间的相关联度和令方差均一化，由于图像中相邻像素之间具有很强的相关性，所以用于训练时很多输入是冗余的。这时候去相关的操作就可以采用自化操作，从而使得：</p>
<ul>
<li>1、减少特征之间的相关性</li>
<li>2、特征具有相同的方差</li>
</ul>
</blockquote>
<h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p>卷积层是构建卷积神经网络的<strong>核心层</strong>，因此该层也是学习的重点。它产生了网络中大部分的<strong>计算量</strong>，注意是计算量而不是参数量。</p>
<p><strong>卷积层的作用</strong></p>
<p>主要有三点：</p>
<ul>
<li><p>滤波器的作用或者说是卷积的作用</p>
<blockquote>
<p>卷积层的参数是有一些可学习的<code>滤波器</code>（<code>Filter</code>）集合构成的。每个滤波器在空间上（宽度和高度）都比较小，<strong>但是深度和输入数据一致</strong>。<br>直观地来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状、车轮状的图案。</p>
</blockquote>
</li>
<li><p>可以看做神经元的一个输出</p>
<blockquote>
<p>神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）。</p>
</blockquote>
</li>
<li><p>减少参数数量</p>
<blockquote>
<p>这个由于卷积具有“<code>权值共享</code>”这样的特性，可以降低参数数量，达到降低计算开销，防止由于参数过多而造成过拟合。</p>
</blockquote>
</li>
</ul>
<p><strong>感受野</strong></p>
<p>在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。而是让每个神经元只与输入数据的一个局部区域连接，该连接的空间大小叫做神经元的<code>感受野（receptive field）</code>，它的尺寸是一个超参数（滤波器的空间尺寸）。注意：在深度方向上，这个连接的大小总是和输入的深度相等。</p>
<p><strong>神经元的空间排列</strong></p>
<p>感受野讲解了卷积层中每个神经元与<strong>输入数据体</strong>之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：<code>深度</code>（<code>depth</code>），<code>步长</code>（<code>stride</code>）和<code>零填充</code>（<code>zero-padding</code>）：</p>
<ul>
<li>深度：一个超参数，和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西，即图像的某些特征。</li>
<li>步长：在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素；当步长为2，滤波器滑动时每次移动2个像素，依次类推。这个操作会让输出数据体在空间上变小。</li>
<li>零填充：有时候将输入数据体用<code>0</code>在边缘处进行填充是很方便的。零填充的尺寸是一个超参数。它具有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，使得输入和输出的宽高都相等）。</li>
</ul>
<p>输出数据体在空间上的尺寸 <code>W2*H2*D2</code>可以通过输入数据体尺寸<code>W1*H1*D1</code>、卷积层中神经元的感受野尺寸<code>F</code>、步长<code>S</code>、滤波器数量<code>K</code>和零填充的数量<code>P</code>计算输出出来：</p>
<p><img src="http://ggb0n.cool/images/size.jpg" alt=""></p>
<p>注意这些空间排列的超参数之间是相互限制的。举例说来，当输入尺寸<code>W=10</code>，不使用零填充<code>P=0</code>，滤波器尺寸 <code>F=3</code>，此时步长<code>S=2</code>是行不通，因为<code>(W-F+2P)/S+1=(10-3+0)/2+1=4.5</code>，结果不是整数，这就是说神经元不能整齐对称地滑过输入数据体。<br>因此，这些超参数的设定就被认为是无效的，一个卷积神经网络库可能会报出一个错误，通过修改零填充值、修改输入数据体尺寸，或者其他什么措施来让设置合理。合理地设置网络的尺寸让所有的维度都能正常工作，是相当让人头痛的事，而使用零填充和遵守其他一些设计策略将会有效解决这个问题。</p>
<p><strong>权值共享</strong></p>
<p>在卷积层中<code>权值共享</code>是用来控制参数的数量。假如在一个卷积核中，每一个感受野采用的都是不同的权重值（卷积核的值不同），那么这样的网络中参数数量将是十分巨大的。</p>
<p>权值共享是基于这样的一个合理的假设：如果一个特征在计算某个空间位置<code>(x1,y1)(x1,y1)</code>的时候有用，那么它在计算另一个不同位置<code>(x2,y2)(x2,y2)</code>的时候也有用。基于这个假设，可以显著地减少参数数量。<br>换言之，就是将深度维度上一个单独的2维切片看做<code>深度切片</code>（<code>depth slice</code>），比如一个数据体尺寸为<code>[55x55x96]</code>的就有96个深度切片，每个尺寸为<code>[55x55]</code>，其中在每个深度切片上的结果都使用同样的权重和偏差获得的。<br>在这样的参数共享下，假如一个例子中的第一个卷积层有96个卷积核，那么就有96个不同的权重集了，一个权重集对应一个深度切片，如果卷积核的大小是<code>11x11</code>的，图像是RGB3通道的，那么就共有<code>96x11x11x3=34,848</code>个不同的权重，总共有<code>34,944</code>个参数（因为要+96个偏差），并且在每个深度切片中的<code>55x55</code>的结果使用的都是同样的参数。</p>
<p>在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。原理如下图：</p>
<p><img src="http://ggb0n.cool/images/CNN02.png" alt=""></p>
<p><img src="http://ggb0n.cool/images/CNN03.png" alt=""></p>
<p>如上两图所示，左侧的神经元是将每一个感受野展开为一列之后串联起来（就是展开排成一列，同一层神经元之间不连接）。右侧的<code>Deep1i</code>是深度为<code>1</code>的神经元的第<code>i</code>个，<code>Deep2i</code>是深度为<code>2</code>的神经元的第<code>i</code>个，同一个深度的神经元的权值都是相同的，黄色的都是相同的（上面4个与下面4个的参数相同），蓝色也都是相同的。</p>
<p>所以现在回过头来看上面说的卷积神经网络的反向传播公式对梯度进行累加求和也是基于这点考虑（同一深度的不同神经元共用一组参数，所以累加）；而每个切片只更新一个权重集的原因也是这样的，因为从图中可以看到，不同深度的神经元不会公用相同的权重，所以只能更新一个权重集。</p>
<blockquote>
<p>注意：如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的卷积（这就是“<code>卷积层</code>”名字由来）。这也是为什么总是将这些权重集合称为滤波器（或<code>卷积核</code>（<code>kernel</code>）），因为它们和输入进行了卷积。</p>
</blockquote>
<blockquote>
<p>注意：有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征（而一个卷积核滑动地与图像做卷积都是在学习相同的特征）。一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心，而我们期望在不同的位置学习到不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为局部连接层（<code>Locally-Connected Layer</code>）。</p>
</blockquote>
<p><strong>卷积层的超参数及选择</strong></p>
<p>由于参数共享，每个滤波器包含<code>F*F*D1</code>个权重，卷积层一共有<code>F*F*D1*K</code>个权重和<code>K</code>个偏移。在输出数据体中，第<code>d</code>个深度切片（空间尺寸是<code>W2*H2</code>），用第<code>d</code>个滤波器和输入数据进行有效卷积运算的结果（使用步长<code>S</code>），最后在加上第<code>d</code>个偏差。</p>
<p>对这些超参数，常见的设置是<code>F=3</code>，<code>S=1</code>，<code>P=1</code>，<code>F=3</code>，<code>S=1</code>，<code>P=1</code>。</p>
<p><strong>卷积层演示</strong></p>
<p>因为3D数据难以可视化，所以所有的数据（<code>输入数据体是蓝色</code>，<code>权重数据体是红色</code>，<code>输出数据体是绿色</code>）都采取将深度切片按照列的方式排列展现。输入数据体的尺寸是<code>W1=5</code>，<code>H1=5</code>，<code>D1=3</code>，<code>W1=5</code>，<code>H1=5</code>，<code>D1=3</code>，卷积层参数<code>K=2</code>，<code>F=3</code>，<code>S=2</code>，<code>P=1</code>，<code>K=2</code>，<code>F=3</code>，<code>S=2</code>，<code>P=1</code>。就是说，有<code>2</code>个滤波器，滤波器的尺寸是<code>3*33*3</code>，它们的步长是<code>2</code>。因此，输出数据体的空间尺寸是<code>(5−3+2)/2+1=3(5−3+2)/2+1=3</code>。注意输入数据体使用了零填充<code>P=1</code>，所以输入数据体外边缘一圈都是<code>0</code>。<br>下面的例子在绿色的输出激活数据上循环演示，展示了其中每个元素都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后求其总和，最后加上偏差得来。<br><img src="http://ggb0n.cool/images/juanji.jpg" alt=""></p>
<p><strong>卷积操作形式</strong></p>
<p>卷积操作的形式比较多，这里主要介绍三点：<code>矩阵乘法实现卷积</code>、<code>1*1卷积</code>、<code>扩张卷积</code>。</p>
<ul>
<li>用矩阵乘法实现卷积</li>
</ul>
<blockquote>
<p>卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法。</p>
<p>(1) 输入图像的局部区域被<code>im2coim2col</code>操作拉伸为列。比如输入是<code>[227x227x3]</code>，要与尺寸为<code>11x11x3</code>的滤波器以步长为<code>4</code>进行卷积，就依次取输入中的<code>[11x11x3]</code>数据块，然后将其拉伸为长度为<code>11x11x3=363</code>的列向量。重复进行这一过程，因为步长为<code>4</code> ，所以经过卷积后的宽和高均为<code>(227-11)/4+1=55</code>，共有<code>55x55=3,025</code>个神经元。因为每一个神经元实际上都是对应有<code>363</code>的列向量构成的感受野，即一共要从输入上取出<code>3025</code>个<code>363</code>维的列向量。所以经过<code>im2col</code>操作得到的输出矩阵的尺寸是<code>[363x3025]</code>，其中每列是拉伸的感受野。注意因为感受野之间有重叠，所以输入数据体中的数字在不同的列中可能有重复。</p>
<p>(2) 卷积层的权重也同样被拉伸成行。举例：如果有96个尺寸为<code>[11x11x3]</code>的滤波器，就生成一个矩阵，尺寸为<code>[96x363]</code>。</p>
<p>(3) 现在卷积的结果和进行一个大矩阵乘法<code>np.dot(Wrow,Xcol)np.dot(Wrow,Xcol)</code>是等价的了，能得到每个滤波器和每个感受野间的点积。在我们的例子中，这个操作的输出是<code>[96x3025]</code>，给出了每个滤波器在每个位置的点积输出。注意其中的<code>np.dotnp.dot</code>计算的是矩阵乘法而不是点积。</p>
<p>(4) 结果最后必须被重新变为合理的输出尺寸<code>[55x55x96]</code>。</p>
</blockquote>
<p>这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在<code>XcolXcol</code>中被复制了多次；优点在于矩阵乘法有非常多的高效底层实现方式。</p>
<ul>
<li>1*1卷积</li>
</ul>
<p>具有信号处理专业知识的人刚开始看见这个<code>1*1卷积</code>的时候可能会比较困惑，因为信号是<code>2</code>维的，所以<code>1*1卷积</code>就没有意义。但是，在卷积神经网络中不是这样，因为这里是对<code>3</code>个维度进行操作，滤波器和输入数据体的深度是一样的。比如，如果输入是<code>[32x32x3]</code>，那么<code>1*1卷积</code>就是在高效地进行<code>3</code>维<code>点积</code>（因为输入深度是3个通道）；另外的一种想法是将这种卷积的结果看作是全连接层的一种实现方式，后面讲到全连接层会提到。</p>
<ul>
<li>扩张卷积</li>
</ul>
<p>我们前面提到的滤波器都是连续的，但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张，如图：</p>
<p><img src="http://ggb0n.cool/images/kuozhang.jpg" alt=""></p>
<p>在某些设置中，扩张卷积与正常卷积结合起来非常有用，因为这可以在很少的层数内更快地汇集输入图片的大尺度特征。比如，如果上下重叠<code>2</code>个<code>3*3</code>的卷积层，那么第二个卷积层的神经元的感受野是输入数据体中<code>5*5</code>的区域（可以称这些神经元的有效感受野是5*5）。如果我们对卷积进行扩张，那么这个有效感受野就会迅速增长。</p>
<p>至此，对卷积层的学习告一段落，下一层是池化层。</p>
<h4 id="ReLU层"><a href="#ReLU层" class="headerlink" title="ReLU层"></a>ReLU层</h4><p>也称线性整流层（<code>Rectified Linear Units layer, ReLU layer</code>），使用<a href="https://zh.wikipedia.org/wiki/线性整流函数">线性整流函数</a>（<code>Rectified Linear Units, ReLU</code>）<code>f(x)=max(0,x)</code>作为这一层神经的激励函数（<code>Activation function</code>）。它可以增强判定函数和整个神经网络的非线性特性，而本身并不会改变卷积层。</p>
<p>事实上，其他的一些函数也可以用于增强网络的非线性特性，如<a href="https://zh.wikipedia.org/wiki/双曲正切函数">双曲正切函数</a> <code>f(x)=tanh(x)</code>、<code>f(x)=|tanh(x)|</code>，或者<a href="https://zh.wikipedia.org/wiki/S函数">Sigmoid函数</a><code>f(x)=(1+e^(-x))^(-1)</code>。相比其它函数来说，ReLU函数更受青睐，这是因为它可以将神经网络的训练速度提升数倍，而并不会对模型的泛化准确度造成显著影响。</p>
<h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>通常在连续的卷积层之间会周期性地插入一个<code>池化层</code>（<code>Pooling</code>），它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。如下图：</p>
<p><img src="http://ggb0n.cool/images/pooling.png" alt=""></p>
<p>池化层使用<code>MAX</code>操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是使用尺寸<code>2*2</code>的滤波器，以步长为<code>2</code>来对每个深度切片进行降采样，将其中<code>75%</code>的激活信息都丢掉。每个<code>MAX</code>操作是从<code>4</code>个数字中取最大值（也就是在深度切片中某个<code>2*2</code>的区域），深度保持不变。</p>
<p>池化层的计算：输入数据体尺寸<code>W1*H1*D1</code>，有两个超参数：空间大小<code>F</code>和步长<code>S</code>；输出数据体的尺寸<code>W2*H2*D2</code>，其中：</p>
<p><img src="http://ggb0n.cool/images/poolingcalc.jpg" alt=""></p>
<p>这里面与之前的卷积的尺寸计算的区别主要在于两点：首先在池化的过程中基本不会进行另补充；其次池化前后深度不变。</p>
<p><strong>普通池化（General Pooling）</strong>：除了常用的最大池化，池化单元还可以使用其他的函数，比如<code>平均池化</code>（<code>average pooling</code>）或<code>L-2范式池化</code>（<code>L2-norm pooling</code>）。平均池化历史上比较常用，但是现在已经很少使用了。因为实践证明，最大池化的效果比平均池化要好。</p>
<p><strong>反向传播</strong>：回顾一下反向传播的内容，其中<code>max(x,y)</code>函数的反向传播可以简单理解为将梯度只沿最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大元素的索引记录下来（有时这个也叫作<code>道岔</code>（<code>switches</code>）），这样在反向传播的时候梯度的路由就很高效。</p>
<p><strong>不使用池化层</strong>：有些时候，被当做图像处理的问题其实也并不完全等同于图像处理，比如AlphaGo的下围棋：棋盘是不能摘除一部分位置进行缩小的，因此也就不能进行池化，事实上AlphaGo也并没有采用池化层。<br>通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为有时候，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如<code>变化自编码器</code>（<code>VAEs：variational autoencoders</code>）和<code>生成性对抗网络</code>（<code>GANs：generative adversarial networks</code>）。未来的卷积网络结构中，池化层的发展还真不能确定。</p>
<h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p>全连接层（简称<code>FC</code>）将每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。全连接层可以整合卷积层或者池化层中具有类别区分性的局部信息。</p>
<p>为了提升 CNN 网络性能，全连接层每个神经元的激励函数一般采用ReLU函数。最后一层全连接层的输出值被传递给一个输出，可以采用<code>softmax</code>逻辑回归（<code>softmax regression</code>）进行分类，该层也可称为<code>softmax</code>层（<code>softmax laye</code>）。</p>
<p><strong>将卷积层转化为全连接层</strong></p>
<p>对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。该全连接层的权重是一个巨大的矩阵，除了某些特定块（感受野），其余部分都是<code>0</code>；而在非<code>0</code>部分中，大部分元素都是相等的（权值共享）。<br>如果把全连接层转化成卷积层，以输出层的<code>Deep11</code>为例，与它有关的输入神经元只有上面四个，所以在权重矩阵中与它相乘的元素，除了它所对应的<code>4</code>个，剩下的均为<code>0</code>，这也就解释了为什么权重矩阵中有为零的部分。<br>另外要把“<code>将全连接层转化成卷积层</code>”和“<code>用矩阵乘法实现卷积</code>”区别开，这两者是不同的，后者本身还是在计算卷积，只不过将其展开为矩阵相乘的形式，并不是”将全连接层转化成卷积层”，所以除非权重中本身有零，否则用矩阵乘法实现卷积的过程中不会出现值为<code>0</code>的权重。</p>
<p><strong>将全连接层转化为卷积层</strong></p>
<p>任何全连接层都可以被转化为卷积层。比如，一个<code>K=409</code>6的全连接层，输入数据体的尺寸是 <code>7*7*5127*7*512</code>，这个全连接层可以被等效地看做一个<code>F=7</code>，<code>P=0</code>，<code>S=1</code>，<code>K=4096</code>，<code>F=7</code>，<code>P=0</code>，<code>S=1</code>，<code>K=4096</code>的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸设为一致的。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成<code>1*1*40961*1*4096</code>，这个结果就和使用初始的那个全连接层一样了。<br>这个实际上也很好理解，因为，对于其中的一个卷积滤波器，这个滤波器的的深度为<code>512</code>，也就是说，虽然这个卷积滤波器的输出只有<code>1</code>个，但是它的权重有<code>7*7*5127*7*512</code>，相当于卷积滤波器的输出为一个神经元，这个神经元与上一层的所有神经元相连接，而这样与前一层所有神经元相连接的神经元一共有<code>4096</code>个，这不就是一个全连接网络。</p>
<p>在上述的两种变换中，<code>将全连接层转化为卷积层</code>在实际运用中更加有用。假设一个卷积神经网络的输入是<code>224*224*3</code>的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为<code>7*7*512</code>的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为<em>224/2/2/2/2/2=7</em>）。从这里可以看到，AlexNet使用了两个尺寸为<code>4096</code>的全连接层，最后一个有<code>1000</code>个神经元的全连接层用于计算分类评分。我们可以将这3个全连接转化为3个卷积层：</p>
<blockquote>
<p>(1) 针对第一个连接区域是<code>[7x7x512]</code>的全连接层，令其滤波器尺寸为<code>F=7</code>，这样输出数据体就为<code>[1x1x4096]</code>了。</p>
<p>(2) 针对第二个全连接层，令其滤波器尺寸为<code>F=1</code>，这样输出数据体为<code>[1x1x4096]</code>。</p>
<p>(3) 对最后一个全连接层也做类似的，令其<code>F=1</code>，最终输出为<code>[1x1x1000]</code>。</p>
</blockquote>
<p>这样做的目的是让卷积网络在一张更大的输入图片上滑动，得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。</p>
<p>至此，对CNN各层的学习暂时告一段落，最起码有了一定的了解。下面将对各层之间的结构进行学习。</p>
<h3 id="卷积神经网络结构特点"><a href="#卷积神经网络结构特点" class="headerlink" title="卷积神经网络结构特点"></a>卷积神经网络结构特点</h3><p>卷积神经网络通常是由三种层构成：卷积层，池化层（除非特别说明，一般就是最大值池化）和全连接层。ReLU层通常在卷积层之后，它逐元素地进行激活函数操作，常常将它与卷积层看作是同一层。</p>
<h4 id="层的排列规律"><a href="#层的排列规律" class="headerlink" title="层的排列规律"></a>层的排列规律</h4><p>卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟池化层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：</p>
<p><img src="http://ggb0n.cool/images/pailie.jpg" alt=""></p>
<p>其中<code>*</code>指的是重复次数，<code>POOL?</code>指的是一个可选的池化层。其中<code>N&gt;=0</code>,通常<code>N&lt;=3</code>，<code>M&gt;=0</code>，<code>K&gt;=0</code>，<code>K&lt;3</code>。例如，下面是一些常见的网络结构规律：</p>
<ul>
<li><strong>INPUT -&gt; FC</strong> ：实现一个线性分类器，此处<code>N = M = K = 0</code>；</li>
<li><strong>INPUT -&gt; CONV -&gt; RELU -&gt; FC</strong>：单层的卷积神经网络；</li>
<li><strong>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC</strong>：此处在每个汇聚层之间有一个卷积层，这种网络就是简单的多层的卷积神经网络；</li>
<li><strong>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]*3 -&gt; [FC -&gt; RELU]*2 -&gt; FC</strong>：此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征。</li>
</ul>
<h4 id="卷积层的大小选择"><a href="#卷积层的大小选择" class="headerlink" title="卷积层的大小选择"></a>卷积层的大小选择</h4><p><strong>几个小滤波器卷积层的组合比一个大滤波器卷积层好</strong>。假设你一层一层地重叠了<code>3</code>个<code>3*3</code>的卷积层（层与层之间有ReLU激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个<code>3*3</code>的视野。第二个卷积层上的神经元对第一个卷积层有一个<code>3*3</code>的视野，也就是对输入数据体有<code>5*5</code>的视野。同样，在第三个卷积层上的神经元对第二个卷积层有<code>3*3</code>的视野，也就是对输入数据体有<code>7*7</code>的视野。</p>
<p>假设不采用这<code>3</code>个<code>3*3</code>的卷积层，而是使用一个单独的有<code>7*7</code>的感受野的卷积层，那么所有神经元的感受野也是<code>7*7</code>。多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。但是也会有一些缺点，假设所有的数据有<code>C</code>个通道，那么单独的<code>7*7</code>卷积层将会包含<code>C*(7*7*C)=49C^2</code>个参数，而<code>3</code>个<code>3*3</code>的卷积层的组合仅有<code>3*(C*(3*3*C))=27C^2</code>个参数</p>
<p>直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。</p>
<h4 id="层的尺寸设置"><a href="#层的尺寸设置" class="headerlink" title="层的尺寸设置"></a>层的尺寸设置</h4><p>各层常见的尺寸设置如下：</p>
<ul>
<li><strong>输入层</strong>：应该能被<code>2</code>整除很多次。常用数字包括<code>32</code>，<code>64</code>，<code>96</code>或<code>224</code>（比如ImageNet卷积神经网络），<code>384</code>和<code>512</code>。</li>
<li><strong>卷积层</strong>：应该使用小尺寸滤波器（比如<code>3*3</code>或最多<code>5*5</code>），使用步长<code>S=1</code>。还有一点非常重要，就是对输入数据进行<code>零填充</code>，这样卷积层就不会改变输入数据在空间维度上的尺寸。比如，当<code>F=3</code>，那就使用<code>P=1</code>来保持输入尺寸。当<code>F=5</code>，<code>P=2</code>，一般对于任意<code>F</code>，当<code>P=(F-1)/2</code>的时候能保持输入尺寸。如果必须使用更大的滤波器尺寸（比如<code>7*7</code>之类），通常只用在第一个面对原始图像的卷积层上。</li>
<li><strong>池化层</strong>：负责对输入数据的空间维度进行降采样。最常用的设置是用<code>2*2</code>感受野（即<code>F=2</code>）的最大值池化，步长为<code>S=2</code>。注意这一操作将会把输入数据中<code>75%</code>的激活数据丢弃（因为对宽度和高度都进行了<code>2</code>的下采样）。<br>另一个不那么常用的设置是使用<code>3*3</code>的感受野，步长为<code>2</code>。最大值池化的感受野尺寸很少有超过<code>3</code>的，因为池化操作过度，易造成数据信息丢失，这通常会导致算法性能变差。</li>
</ul>
<p>至此，对卷积神经网络的学习到此告一段落，很多深层次的地方其实还没搞懂，在以后的实践中再加深学习吧。</p>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/37261854">卷积神经网络入门详解</a><br><a href="https://www.bilibili.com/video/BV1hp411d7ij/?spm_id_from=333.788.videocard.0">李宏毅机器学习教学视频</a></p>
</div><ul class="post-copyright"><li><strong>Post Title: </strong><a href="http://ggb0n.cool/2020/04/09/卷积神经网络CNN学习记录/">卷积神经网络CNN入门</a></li><li><strong>Post Author: </strong><a href="http://ggb0n.cool">ggb0n</a></li><li><strong>Post Link: </strong><a href="http://ggb0n.cool/2020/04/09/卷积神经网络CNN学习记录/">http://ggb0n.cool/2020/04/09/卷积神经网络CNN学习记录/</a></li><li><strong>Copyright Notice: </strong><span>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></li></ul><div><div class="recommend-post"><span class="is-size-6 has-text-grey has-mr-7"># Related Post</span><br><span>  1.<a class="is-size-6" href="/2020/04/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/" target="_blank">机器学习基本知识</a><br></span></div><div class="recommend-post"><span class="is-size-6 has-text-grey has-mr-7"># Recommend Post</span><br><span>  1.<a class="is-size-6" href="/2020/04/16/obfs4%E7%BD%91%E6%A1%A5%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/" target="_blank">obfs4网桥问题的处理</a><br></span></div></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/social-share.js/1.0.16/css/share.min.css"><div class="social-share"></div><script src="https://cdnjs.loli.net/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="http://ggb0n.cool/images/emFuc2hhbmc=.png" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/04/16/obfs4%E7%BD%91%E6%A1%A5%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">obfs4网桥问题的处理</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/04/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/"><span class="level-item">机器学习基本知识</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.0/gitalk.css"><script> $.getScript('/js/gitalk.min.js', function () { 
            var gitalk = new Gitalk({
            id: 'f8c2b0786d3cfb6dc31c18f16977f3f2',
            repo: 'blog_comment',
            owner: 'ggb0n',
            clientID: '0a75c84e24da4e774d10',
            clientSecret: '80c72733da730b019e5047ae6c7eff937914a4d9',
            admin: ["ggb0n"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            
            
            enableHotKey: true,
            isLocked: false
        })
        gitalk.render('comment-container')});</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget toc-scroll" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list toc"><li><a class="is-flex toc-item" id="toc-item-卷积神经网络概述" href="#卷积神经网络概述"><span>卷积神经网络概述</span></a></li><li><a class="is-flex toc-item" id="toc-item-卷积神经网络层次" href="#卷积神经网络层次"><span>卷积神经网络层次</span></a><ul class="menu-list toc"><li><a class="is-flex toc-item" id="toc-item-输入层" href="#输入层"><span>输入层</span></a></li><li><a class="is-flex toc-item" id="toc-item-卷积层" href="#卷积层"><span>卷积层</span></a></li><li><a class="is-flex toc-item" id="toc-item-ReLU层" href="#ReLU层"><span>ReLU层</span></a></li><li><a class="is-flex toc-item" id="toc-item-池化层" href="#池化层"><span>池化层</span></a></li><li><a class="is-flex toc-item" id="toc-item-全连接层" href="#全连接层"><span>全连接层</span></a></li></ul></li><li><a class="is-flex toc-item" id="toc-item-卷积神经网络结构特点" href="#卷积神经网络结构特点"><span>卷积神经网络结构特点</span></a><ul class="menu-list toc"><li><a class="is-flex toc-item" id="toc-item-层的排列规律" href="#层的排列规律"><span>层的排列规律</span></a></li><li><a class="is-flex toc-item" id="toc-item-卷积层的大小选择" href="#卷积层的大小选择"><span>卷积层的大小选择</span></a></li><li><a class="is-flex toc-item" id="toc-item-层的尺寸设置" href="#层的尺寸设置"><span>层的尺寸设置</span></a></li></ul></li></ul></div></div><script type="text/javascript" async>
        $(document).ready(function () { //参考自 https://github.com/ppoffice/hexo-theme-icarus/pull/616/files
            var observerTopMargin;
            var scrollObserver;
            var headerElems = $(".headerlink");
            var activeTocItem;
        
            function initIntersectionObserver(docHeight) {
                observerTopMargin = docHeight;
                scrollObserver = new IntersectionObserver(scrollCallBack,
                    {
                        root: null,  // viewpoint
                        rootMargin: docHeight + "px 0px -80% 0px"  // cover top 30% of viewport to the top of document
                    })
            }
        
            function scrollCallBack(entries, observer) {
                if ($(window).scrollTop() > observerTopMargin * 0.7) { 
                    // User somehow scroll to 70% of observerTopMargin (which is inited as 200% document height)
                    // Observer top margin need to extend to cover all the space to the top of the document
                    initIntersectionObserver(observerTopMargin * 2)
                    observer.disconnect();
                    return;
                }
                let toActive;
                if (entries[0].intersectionRatio == 1) {  // enter viewed area
                    let entry = entries.reduce((u, v) => (u.target.toc_id > v.target.toc_id ? u : v));  // get the lowest item
                    toActive = $("#toc-item-" + $(entry.target).attr("href").substr(1));
                } else {
                    let entry = entries.reduce((u, v) => (u.target.toc_id < v.target.toc_id ? u : v));  // get the highest item
                    let idx = Math.max(entry.target.toc_id - 1, 0);
                    toActive = $("#toc-item-" + $(headerElems[idx]).attr("href").substr(1));
                }
                if (activeTocItem) activeTocItem.removeClass("is-current");
                activeTocItem = toActive
                activeTocItem.addClass("is-current");
            }
        
            initIntersectionObserver($(document).height() * 2);
            headerElems.each(function (index, obj) {
                obj.toc_id = index;
                scrollObserver.observe(obj);
            })
        });</script></div><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="http://ggb0n.cool/images/header2.jpg" alt="ggb0n"></figure><p class="title is-size-4 is-block line-height-inherit">ggb0n</p><p class="is-size-6 is-block">一生一世，爱你一人。</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>伟大的中国</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">24</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">92</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ggb0n" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ggb0n"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://weibo.com/u/5955623314"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:gg.b0n@qq.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Next" href="https://15h3na0.xyz"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"来源《"+data.from+"》</p><p>提供者-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Latest Comment</h3><span class="body_hot_comment">Loading...Wait a Minute!</span></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><a class="media-left" href="/2020/04/16/obfs4%E7%BD%91%E6%A1%A5%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/"><p class="image is-64x64"><img class="thumbnail" src="http://ggb0n.cool/images/wangqiao-top1.jpg" alt="obfs4网桥问题的处理"></p></a><div class="media-content size-small"><p><time dateTime="2020-04-16T01:52:29.000Z">2020-04-16</time></p><p class="title is-6"><a class="link-muted" href="/2020/04/16/obfs4%E7%BD%91%E6%A1%A5%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/">obfs4网桥问题的处理</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9E%8E%E6%8D%89%E6%91%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0/">瞎捉摸的一些学习</a> / <a class="link-muted" href="/categories/%E7%9E%8E%E6%8D%89%E6%91%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0/obfs4%E7%BD%91%E6%A1%A5%E6%90%AD%E5%BB%BA/">obfs4网桥搭建</a></p></div></article><article class="media"><a class="media-left" href="/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"><p class="image is-64x64"><img class="thumbnail" src="http://ggb0n.cool/images/cnn-top.jpg" alt="卷积神经网络CNN入门"></p></a><div class="media-content size-small"><p><time dateTime="2020-04-08T16:16:23.000Z">2020-04-09</time></p><p class="title is-6"><a class="link-muted" href="/2020/04/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/">卷积神经网络CNN入门</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><a class="media-left" href="/2020/04/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/"><p class="image is-64x64"><img class="thumbnail" src="http://ggb0n.cool/images/ml-top.jpg" alt="机器学习基本知识"></p></a><div class="media-content size-small"><p><time dateTime="2020-04-08T16:03:30.000Z">2020-04-09</time></p><p class="title is-6"><a class="link-muted" href="/2020/04/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/">机器学习基本知识</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><a class="media-left" href="/2020/04/03/FTP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%E5%8F%8A%E5%85%B8%E5%9E%8B%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/"><p class="image is-64x64"><img class="thumbnail" src="http://ggb0n.cool/images/ftp-top.jpg" alt="FTP服务器搭建及典型问题的解决"></p></a><div class="media-content size-small"><p><time dateTime="2020-04-03T01:59:36.000Z">2020-04-03</time></p><p class="title is-6"><a class="link-muted" href="/2020/04/03/FTP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%E5%8F%8A%E5%85%B8%E5%9E%8B%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/">FTP服务器搭建及典型问题的解决</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9E%8E%E6%8D%89%E6%91%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0/">瞎捉摸的一些学习</a> / <a class="link-muted" href="/categories/%E7%9E%8E%E6%8D%89%E6%91%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0/FTP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA/">FTP服务器搭建</a></p></div></article><article class="media"><a class="media-left" href="/2020/04/01/golang%E7%BC%96%E8%AF%91%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B7%A8%E5%9D%91/"><p class="image is-64x64"><img class="thumbnail" src="http://ggb0n.cool/images/golang-top.png" alt="golang编译项目的一个巨坑"></p></a><div class="media-content size-small"><p><time dateTime="2020-04-01T02:07:31.000Z">2020-04-01</time></p><p class="title is-6"><a class="link-muted" href="/2020/04/01/golang%E7%BC%96%E8%AF%91%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B7%A8%E5%9D%91/">golang编译项目的一个巨坑</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9E%8E%E6%8D%89%E6%91%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0/">瞎捉摸的一些学习</a> / <a class="link-muted" href="/categories/%E7%9E%8E%E6%8D%89%E6%91%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0/obfs4%E7%BD%91%E6%A1%A5%E6%90%AD%E5%BB%BA/">obfs4网桥搭建</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/CTF%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"><span class="level-start"><span class="level-item">CTF学习记录</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/CTF%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/BUU%E5%88%B7%E9%A2%98/"><span class="level-start"><span class="level-item">BUU刷题</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/CTF%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/BUU%E5%88%B7%E9%A2%98/SQL%E6%B3%A8%E5%85%A5/"><span class="level-start"><span class="level-item">SQL注入</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/CTF%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/CTFHub%E5%88%B7%E9%A2%98/"><span class="level-start"><span class="level-item">CTFHub刷题</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/CTF%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E6%AF%94%E8%B5%9B%E5%88%92%E6%B0%B4/"><span class="level-start"><span class="level-item">比赛划水</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/CTF%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%B5%9B%E9%A2%98%E5%A4%8D%E7%8E%B0/"><span class="level-start"><span class="level-item">赛题复现</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%9E%8E%E6%8D%89%E6%91%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">瞎捉摸的一些学习</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/%E7%9E%8E%E6%8D%89%E6%91%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0/FTP%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA/"><span class="level-start"><span class="level-item">FTP服务器搭建</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%9E%8E%E6%8D%89%E6%91%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0/obfs4%E7%BD%91%E6%A1%A5%E6%90%AD%E5%BB%BA/"><span class="level-start"><span class="level-item">obfs4网桥搭建</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/04/"><span class="level-start"><span class="level-item">April 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/03/"><span class="level-start"><span class="level-item">March 2020</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/02/"><span class="level-start"><span class="level-item">February 2020</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/CTF/"><span class="tag">CTF</span><span class="tag is-grey-lightest">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag is-grey-lightest">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL%E6%B3%A8%E5%85%A5/"><span class="tag">SQL注入</span><span class="tag is-grey-lightest">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cookie%E6%94%BB%E5%87%BB/"><span class="tag">Cookie攻击</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bypass-functions-disable/"><span class="tag">bypass functions_disable</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JWT%E4%BC%AA%E9%80%A0/"><span class="tag">JWT伪造</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RSA%E6%94%BB%E5%87%BB/"><span class="tag">RSA攻击</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E6%BC%8F%E6%B4%9E/"><span class="tag">反序列化漏洞</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B8%83%E5%B0%94%E7%9B%B2%E6%B3%A8/"><span class="tag">布尔盲注</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"><span class="tag">文件上传</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RCE/"><span class="tag">RCE</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SSRF/"><span class="tag">SSRF</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SSTI/"><span class="tag">SSTI</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E6%AC%A1%E6%B3%A8%E5%85%A5/"><span class="tag">二次注入</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%A5%E9%94%99%E6%B3%A8%E5%85%A5/"><span class="tag">报错注入</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Neural-Network/"><span class="tag">Neural Network</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RSA%E5%85%B1%E6%A8%A1%E6%94%BB%E5%87%BB/"><span class="tag">RSA共模攻击</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tor/"><span class="tag">Tor</span><span class="tag is-grey-lightest">2</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe to Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=removeifFeedsId&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="removeifFeedsId" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="Subscribe"></div></div><p class="help">输入邮箱开始订阅，更博后邮件通知！</p></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo1.png" alt="ggb0n&#039;s Blog" height="28"></a><p class="size-small"><span>&copy; 2020 ggb0n</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>,Modify by <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">removeif</a> <br><span>© <a href="http://www.beian.miit.gov.cn/" target="_blank">豫ICP备20003023号</a><br></span><span>© 版权说明：[本博客所有内容均学习于互联网或自己原创,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2020/02/02 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ggb0n"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://ggb0n.cool',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back-to-top.js" defer></script><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.12/js/lightgallery-all.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><!--!--><!--!--><!--!--><script src="/js/toc.js" defer></script><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('0a75c84e24da4e774d10','80c72733da730b019e5047ae6c7eff937914a4d9','ggb0n','blog_comment',false);})</script><link rel="stylesheet" href="/css/insight.css"><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="Type something..."><span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>(function (window) {
            var INSIGHT_CONFIG = {
                TRANSLATION: {
                    POSTS: 'Posts',
                    PAGES: 'Pages',
                    CATEGORIES: 'Categories',
                    TAGS: 'Tags',
                    UNTITLED: '(Untitled)',
                },
                CONTENT_URL: '/content.json',
            };
            window.INSIGHT_CONFIG = INSIGHT_CONFIG;
        })(window);</script><script src="/js/insight.js" defer></script></body></html>